# 目标检测算法总结

​	目前，人类具有看一下图像就知道图像中有哪些物体，这些物体的位置大概在哪些位置的能力，这种能力是我们天生就有的，潜意识就执行的能力，这种能力使得我们可以对自己看到的景象进行实时的判断，使得我们可以行走，驾驶等等。但是，这种能力计算机不具备，如果想要计算机具有与人类似的能力就要借助目标检测方法来实现。用于物体检测的快速精确算法将允许计算机在没有专用传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时场景信息，并释放通用响应机器人系统的潜力。此外，目标检测也是计算机视觉领域中的重要部分。

​	目前的目标检测算法主要分为两个分支，分为**two-stage**和**one-stage**。

​	其中，two-stage的目标检测方法是根据**classesifier分类器**演化来的，主要是通过不同的**region proposals方法进行ROI的提取**，然后再利用不同的网络对提取出的**BBoxes进行回归定位和分类**，最后经过**后处理**如NMS非极大值抑制等方法进行BBox的去重。two-stage方法步骤复杂，而且方法中的每个独立的部分都要被单独训练，导致这类方法速度的**速度很慢**。（这类方法的整体结构分成几次独立的部分，可以对每个部分单独进行优化）。

​	one-stage的目标检测算法是**直接对输入图像应用算法并输出目标的定位和所属类别**，代表方法为YOLO系列。

目标检测模型进化史及简单性能对比：

![](D:\typora图片总\目标检测模型进化史.png)



## 一、R-CNN



## 二、SPP-Net



## 三、Fast R-CNN



## 四、Faster R-CNN



## 五、R-FCN



## 六、YOLO​	

### 1、YOLOv1

#### （1）概述

​	YOLO是一个**单一的、整体的网络**，属于one-stage方法，其将目标检测问题重新构造成一个回归regression问题，直接从一整张图像来预测出bounding box 的坐标、bbox中包含物体的置信度和物体的probabilities。因为YOLO的物体检测流程是在一个神经网络里完成的，所以可以end to end来优化物体检测性能。

**下图是各物体检测系统的检测流程对比：**

![](D:\typora图片总\各物体检测系统的检测流程对比.png)

- **YOLO的优点：**

1、YOLO检测物体的速度很快，基本上能够达到实时的标准，如标准版本的YOLO在Titan X 的 GPU 上能达到45 FPS。网络较小的版本Fast YOLO在保持mAP是之前的其他实时物体检测器的两倍的同时，检测速度可以达到155 FPS。同时能够达到其他实时算法mAP的两倍。

2、相较于其他的state-of-the-art 物体检测系统，YOLO在物体定位时更容易出错，但是在背景上预测出不存在的物体（产生false positives）的情况会少一些。而且，YOLO比DPM、R-CNN等物体检测系统能够学到更加抽象的物体的特征，这使得YOLO可以从真实图像领域迁移到其他领域，如艺术。

3、YOLO将two-stage方法中分开的部分用一个单独的卷积神经网络代替，这个神经网络同时进行特征提取、bounding boxes的预测、NMS以及语义推理等。

- **YOLO的缺点：**

1、YOLO的物体检测精度低于其他state-of-the-art的物体检测系统。 

2、YOLO容易产生物体的定位错误。

3、YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能预测2个物体）。

#### （2）实现方法

- YOLO致力于实现一个端到端的网络，来达到实时标准的同时保持较高的准确度。

  ![](D:\typora图片总\YOLO1.png)

- 首先，YOLO将输入先划分成S*S的网格，如果某一个对象的中心落入到一个网格单元中，那么这个栅格单元grid cell就负责这个对象的检测。每个grid cell都需要预测B个bounding boxes（原文中B=2）的坐标以及这些bounding boxes的confidence score置信度。confidence scores是model认为这些box中包含object的可能性以及预测的box坐标的准确度。其计算公式为：

![](D:\typora图片总\置信度公式.png) 

​       在公式中，前者是预测框中是否有目标的概率，后者是预测框和真实框的交并比IOU。

​       其中，若bbox中不包含对象，那么confidence score为0，反之，confidence score就等于the predicted box与the ground truth之间的IOU值。

- 对每个bounding boxes都要预测五个参数值：x，y，w，h，confidence score。坐标x,y代表了预测的bounding box的中心与栅格边界的相对值，坐标w,h代表了预测的bounding box的width、height相对于整幅图像width,height的比例， 最后 confidence score在box中包含对象的时候表示预测的box与真实box之间的IOU。

  ![](D:\typora图片总\YOLO坐标.gif)

- 同时，也要对图像中S*S个网格单元中的每个grid cell预测一个集合大小为C（数据中对象的类数）的类概率集合，不管bboxes的个数。这个class probabilities 是一个条件概率，表示为 **$P_r(Class_i|Object)​$** ，即在一个栅格包含一个Object的前提下，它属于某个类的概率。

  ![](D:\typora图片总\C类概率.jpg)

- 由以上可知，YOLO网络的输出维度为：S\*S（B*5+C）,即共有S\*S个栅格，每个栅格要预测B个bounding boxes的5个参数和C个类概率。例如，对于Pascal VOC数据集（20类）来说，YOLO设S = 7，B = 2，C = 20，此时YOLO网络到最
  后要预测一个7\*7\*(2\*5+20)的tensor。

- 在**测试**阶段，将每个栅格的条件类别概率与每个bboxes的confidence相乘，这样就得到了对于每个bbox的目标类概率以及box预测位置的精准度的描述。**此时的每个栅格的输出是B列20行的概率值**。

![](D:\typora图片总\测试阶段精确度.png)

![](D:\typora图片总\YOLO模型1.png)

![](D:\typora图片总\YOLO2.gif)

![](D:\typora图片总\YOLO3.gif)

- 网络结构

​       网络的设计，主要是通过卷积层来提取图像特征，同时使用全连接层来预测bbox的类与坐标。

​	YOLO网络主要借鉴了GoogleNet的思想，但是与GoogleNet的Inception modules不同，网络中使用1\*1降维层（进行跨通道信息整合）加上3*3的卷积层来代替Inception modules。YOLO网络结构由24个卷积层，2个全连接层构成，网络入口。Fast YOLO有9个卷积层，并且filters的数目更少，用来做快速检测。

![](D:\typora图片总\YOLO网络结构.png)

![](D:\typora图片总\YOLO网络结构2.jpg)

#### （3）训练流程

- 将YOLO网络的前20个卷积层在ImageNet 1000-class dataset上进行预训练，这个预训练网络除了20个卷积层还有一个averagepooling层以及 a fully connected层。训练大约一周的时间，使得在ImageNet 2012的验证数据集Top-5的精度达到 88%，这个结果跟 GoogleNet 的效果相当。原文中的YOLO的实现是在Darknet框架下实现的。

- 在预训练的20个卷积层后面加上随机初始化参数的四个卷积层和两个全连接层用于目标检测任务。由于检测任务需要比较细粒度的图像信息，所以将网络的分辨率从224\*224调整到448*448。

- 将预测的w，h相对于图像的宽度和高度进行标准化，使其介于0和1之间。将bounding box的x，y坐标参数化成相对于网格单元位置的偏离，使他们也介于0和1之间。

  坐标的具体计算：

  ![](D:\typora图片总\坐标计算.png)

  a. 对于上图中蓝色框的单元格，坐标为（xcol=1，yrow=4），假设它预测的输出是红色框的bbox，设bbox的中心坐标为（$X_C,Y_C$），则最终预测出来的（x，y）是经过归一化处理的，表示的是中心相对于单元格的offset，其计算公式如图。

  b. 预测的bbox的宽高为（$W_b,h_b$），（w，h）表示的是bbox的是相对于整体图片的占比，计算公式如图。

- 对于网络的最后一层使用linear activation function线性激活函数，对于网络中的其它层使用
  leaky rectified linear activation function:

![](D:\typora图片总\leaky rectified linear activation function.png)

- 为了防止过拟合，在第一个全连接层后面接了一个ratio=0.5的Dropout层。
- 训练使用的损失函数：

损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。 

Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。

但是定位误差和分类误差的权重不应该相同，因为8维（两个bboxes）的localization error和20维的classification error同等重要显然是不合理的。另外，如果一些栅格中没有object（一幅图中这种栅格很多），那么就会将这些栅格中的bounding box的confidence 置为0，相比于较少的有object的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。最后，平方和误差将大box和小box的权重等同的对待，在big box中的误差可能是微小的，但是相对于small box可能致命的误差损失。

![](D:\typora图片总\YOLO损失函数.png)

解决上述问题的方案为：

a. 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 **$λ_{coord}​$** ,在pascal VOC训练中取5。（上图蓝色框） 

b. 对没有object的bbox的confidence loss，赋予小的loss weight，记为**$λ_{noobj}$**，在pascal VOC训练中取0.5。（上图橙色框）

c. 有object的bbox的confidence loss (上图红色框) 和类别的loss （上图紫色框）的loss weight正常取1。

d. 用box的width和height取平方根代替原本的height和width，使loss能够体现出相同的误差对于big box及small box的重要性不同。

![](D:\typora图片总\YOLO损失函数优化.png)

#### （4）检测流程

![](D:\typora图片总\YOLO使用流程.jpg)

**如上图所示，使用YOLO来检测物体，其基本流程如下：**

a、将图像resize到448 * 448作为神经网络的输入 

b、运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities 

c、进行非极大值抑制，筛选Boxes

**非极大值抑制**：

​	假设YOLO划分成的栅格为7*7，B=2，则共有98个bounding boxes，这相对与SS方法产生2000多个这是非常少的，所以大大加快了网络的运行速度。同时由于是对图像进行网格划分，这样就对bounding box 的位置进行了空间约束，能够很清楚的知道一个对象中心落入哪个网格单元中，并且为那个对象预测唯一的一box。但是一些小目标或者目标相邻很近或者目标离多个网格边界很近，这样就会被多个网格单元进行定位，产生多个bounding box，这就需要使用Nonmaximal suppression(NMS)来进行这些重复检测的去重。

NMS（非极大值抑制的基本步骤）：

a. 首先从所有的检测框中找到置信度最大的那个框

b. 然后挨个计算这个置信度最大框与**所有**剩余框的IOU，如果其值大于一定的阈值（重合度过高），那么将这个框（指剩余框）剔除

c. 之后对剩下的检测框（经a、b步骤剔除框后）重复上述过程（先找最大的，然后和其余的计算IOU,剔除IOU大的框），直至处理完所有的框

注：在测试阶段，是将S\*S*B\*20的结果送入NMS。（每个栅格输出B列20行的概率值，概率是栅格的条件类别概率与每个bboxes的confidence相乘的结果，在实现方法概述中已经提到过）



![](D:\typora图片总\YOLO非极大值抑制.gif)

YOLO的实际策略是先使用NMS，然后再确定各个box的分类，其基本过程如下图：

![](D:\typora图片总\YOLO检测流程.gif)

![](D:\typora图片总\YOLO获取检测结果.gif)

​          （ 注：上面第一张图体现分类别地进行NMS，第二张图体现先进行NMS，在确认bbox的类别）

对于98个bboxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。

#### （5）性能分析

-  先看一下Yolo算法在PASCAL VOC 2007数据集上的性能，与其它检测算法做了对比，包括DPM，R-CNN，Fast R-CNN以及Faster R-CNN。其对比结果如下图所示：

![](D:\typora图片总\YOLO V1性能.png)

与实时性检测方法DPM对比，可以看到Yolo算法可以在较高的mAP上达到较快的检测速度，其中Fast Yolo算法比快速DPM还快，而且mAP是远高于DPM。但是相比Faster R-CNN，Yolo的mAP稍低，但是速度更快。所以。Yolo算法算是在速度与准确度上做了折中。

- 为了进一步分析Yolo算法，文章还做了误差分析，将预测结果按照分类与定位准确性分成以下5类：     

​     Correct：类别正确，IOU>0.5；（准确度）；

​     Localization ：类别正确，0.1 < IOU<0.5（定位不准）；

​     Similar：类别相似，IOU>0.1；

​     Other：类别错误，IOU>0.1；

​     Background：对任何目标其IOU<0.1。（误把背景当物体）

**Yolo与Fast R-CNN的误差对比分析如下图所示：**

![](D:\typora图片总\Fast R-CNN.png)

由此图可以看到，Yolo的Correct低于Fast R-CNN。另外Yolo的Localization误差偏高，即定位不是很准确。但是Yolo的Background误差很低，说明其对背景的误判率较低。

- 具体类AP分析：

![](D:\typora图片总\具体类AP分析.jpg)

-   推广到风格艺术数据集分析：     


![](D:\typora图片总\风格艺术数据集分析.jpg)

- 而且还有对于摄像头实时监测的实验，将YOLO与webcam相连，能够达到实时的性能效果，包括从相机中提取图像与将图像显示出来的时间在内，这样的系统就有些像追踪系统，随着摄像头中目标的运动，监测结果也随之变化。



### 2、YOLOv2/9000

#### （1）概述

​	虽然YOLOV1的检测速度很快，但是在检测精度上却不如RCNN系列检测方法，在物体定位方面（localization）不够准确，并且召回率（recall，查全率）较低。YOLOV2提出了8种改进方法来提升YOLO模型的定位准确率和召回率，从而提高mAP，并且保持检测速度。

​	YOLO V2主要是在YOLO V1的基础上在**精确度**以及**速度**这两个方面进行改进，在精准度方面提出**8个tricks**（技巧）方法来进行改进，在速度方面主要提出了一个更精简的网络架构**Darknet-19**来减少网络中的参数，提升速度，并且对YOLOV1的训练分类器进行特征提取的过程进行了改善，这样也提升了网络的精确度，然后在Darknet-19的基础上构建了检测网络。

​	YOLO V2相比于YOLO V1做了很多方面的改进，这也使得YOLO V2的mAP显著提升，并且保持YOLO V2的速度依然很快，保持了作为OneStage方法的优势，YOLOV2和Fatser RCNN，SSD等模型的对比如图所示：

![](D:\typora图片总\YOLO V2模型对比.jpg)

除了修改网络的tricks，YOLOV2的**两个亮点**如下：

**a**.  YOLOV2致力于进行精度和速度之间的平衡（tradeoff between speed and accuracy），它提出了一个新的multiscale training method (多尺度训练模型)，这样使得相同的YOLOv2模型能够在不同大小的图像（不同尺度的）上运行。(可以输入不同分辨率的图像)。
**b**.  YOLOV2 提出了一个新的方法来在 目标检测和分类任务上进行jointly train（联合训练），从而使得YOLO9000能够同时在COCO检测数据集与ImageNet分类数据集上训练，因此可以对那些没有标注的目标分类进行检测

#### （2）实现方法

YOLO论文主要是从Better、Faster、Stronger这个三个方面的改善来进行论述的，提出了YOLOV2，并在YOLOV2训练的基础上联合训练得到了YOLO9000模型。下面分别就Better、Faster与Stronger这三个方面来讲述在这篇文章中的改进。

##### Better

首先，利用8个tricks来改善YOLO的精度，即提高YOLO的mAP与recall，其中大部分方法都能够显著提升模型的精度，具体的改进侧率如图所示。

![](D:\typora图片总\YOLOV2 改进.png)

由上图可以看到：High Resolution Classifier的提升非常明显（近4%），另外通过结合dimension prior+localtion
prediction这两种方式引入anchor也能带来近5%mAP的提升。

**1、Batch Normalization(BN, 批标准化)**

CNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，但可以通过normalize每层的输入解决这个问题。

简单总结一下BN的作用为：

1. BN能够解决数据偏移：以激活函数ReLU为例max(0,x)，每次经过一个ReLU激活函数，小于0的数据就会置零，大于0的数据就会得到保留，这样从卷积或者全连接层输出的分布就会往数据为正的方向偏移，加入BN就能使数据重新变为[0,1]的正态分布 。 
2. 反向传播的时候会存在梯度消失或者梯度爆炸，例如$0.7^5$=0.168，$1.2^9​$=5.15,网络越深，这种现象越严重。使用BN能使梯度稳定，更容易收敛。 
3. 在BN中，每一个数据都会受到当前Batch内数据的影响。同一个样本，在不同的Batch内，由于BN存在，输出的概率都不一定相同，这样可以认为是提高了模型的泛化能力，这样也就降低了过拟合的可能性。

**在YOLOv2中，作者在每一层卷积层之后都加入BN层，然后去掉了Dropout**，综合起来mAP提高了2%。

**2、High Resolution Classifier**

​	目前大部分检测模型都会在ImageNet分类数据集上训练模型的主体部分（CNN特征提取器）。由于之前ImageNet分类模型基本采用大小为224x224的图片作为输入，分辨率相对较低，**不利于检测模型**，所以YOLO V1网络在预训练的时候采用224\*224的输入，而**在detection检测的时候采用448\*448的输入**。这样的话就会导从分类模型切换到检测模型的时候，模型还要适应图像分辨率的改变。

**而YOLO v2则将预训练分成两步：**

​	先用224\*224的输入从头开始训练网络，大概160个epoch（表示将所有训练数据循环跑160次），然后再将输入调整到448*448，再训练10个epoch。

注意这两步都是在ImageNet数据集上操作。最后再在检测的数据集上fine-tuning，也就是detection的时候用448\*448的图像作为输入就可以顺利过渡了。作者的实验表明这样可以提高几乎4%的MAP。

**3、Convolutionlal With Anchor Boxes**

​	在YOLOv1中，输入图片最终被划分为7x7网格，每个网格单元预测两个bounding box。YOLOV1最后采用的是全连接层直接对bounding box进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比（scales and ratios）的物体，YOLOV1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOV1在精确定位方面表现较差。

​	YOLOV2借鉴了**Faster RCNN中的RPN网络的先验框（anchor boxes, prior boxes, SSD也采用了先验框）策略，**anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。YOLOV2中用了5种类型的Anchor box。

​	RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体），并且各个位置设置不同尺度和比例的先验框，所以**RPN预测的是边界框相对于先验框的offsets值**（其实是transform值，详细见Faster R_CNN论文），采用先验框使得模型更容易学习。如图：

![](D:\typora图片总\RPN网络.png)

​	YOLO v2**去除了YOLO v1中的全连接层而采用了卷积和anchor boxes来预测边界框**（全连接中，4096*7\*7\*30,参数过多，不容易学习）。且为了使检测所用的特征图分辨率更高，移除最后的一个pool层在检测模型中。通过缩减网络，YOLOv2不是采用481\*418图片作为输入，而是采用416\*416大小。因为YOLOv2模型下采样的总步长为32，对于416\*416大小的图片，最终得到的特征图大小为13\*13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。所以在**YOLOv2设计中要保证最终的特征图有奇数个位置。**

​	在YOLO V1中，每个grid cell只预测2个bounding boxes，每个bounding box包含5个值：（x, y, w，h, c），前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。

​	YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）使用anchor boxes之后，YOLOv2的mAP有稍微下降（这里下降的原因，我猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。

​	YOLOv1只能预测98个（7x7x2）边界框，而YOLOv2使用anchor boxes之后可以预测上千个(**13x13x num_anchors**)边界框。所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%。

**总结来说，作者主要采用了以下几点措施**：

- 去掉一个Pooling layer提高了分辨率 。

- 缩小图片从448到416，因为作者采用的darknet到预测这层的时候，尺度缩小了32倍。存在这种先验，很多图片大目标占据中间位置，如果是448/32=14的话，需要用中间4个feature 点进行预测，416/32 = 13，就只会有一个点参与预测。

- 预测类别从空间位置中分开，由anchor box同时预测类别和坐标。YOLOv1每一个cell预测类别，对应2个bounding box预测坐标和置信度。 YOLOv2中，一个Anchor Box直接预测类别，4个坐标位置，和置信度。

![](D:\typora图片总\YOLOV2输出维度.png)



![](D:\typora图片总\YOLOV2输出tensor图.png)

**4、Dimension Clusters**（维度聚类）

​	在Faster RCNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果一开始选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，另外作者发现如果采用标准的k-means（即用欧式距离来衡量差异），在box的尺寸比较大的时候其误差也更大，而我们希望的是误差和box的尺寸没有太大关系。所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标，则最终的距离函数为：

![](D:\typora图片总\k-means距离函数.png)以下为，为在VOC和COCO数据集上的聚类分析结果，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取5个聚类中心作为先验框。对于两个数据集，5个先验框的width和height如下所示（来源：YOLO源码的cfg文件）：

```
COCO: (0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434), (7.8828
2, 3.52778), (9.77052, 9.16828)
VOC: (1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112,
4.84053), (11.2364, 10.0071)
```

但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（13*13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。

![](D:\typora图片总\维度聚类.png)



**5、Direct location prediction（直接位置预测）**

​	由上文可知，YOLOV2借鉴了Faster RCNN中的RPN网络使用anchor boxes来**预测边界框相对于先验框的offsets**， 使用anchor boxes之后会存在模型不稳定的问题，特别是迭代早期。作者发现，这种不稳定来源于预测(x，y)的位置。

​	边界框的实际中心位置（x，y），需要根据预测的坐标偏移值（$t_x，t_y$），先验框的尺度（$w_a,h_a$），先验框的中心坐标（$x_a,y_a$）（特征图每个位置的中心点） 来计算，其计算公式为（根据Faster R-CNN推导出）：

​                                                               $$x = (t_x * w_a) + x_a$$

​                                                               $$y = (t_y * h_a) + y_a$$      **（注：原文中作者错把加号写成了减号）**

​	首先$t_x,t_y​$代表相对于预设固定值的偏移量，例如，$t_x=1​$就会把box向右移动一个预定$w_a​$的距离。$t_x=-1​$就会把box向左移动一个预定$w_a​$的距离。因此每个位置预测的边界框可以落在图片的任何位置，这导致了模型的不稳定性，在训练时需要很
长时间来预测出正确的offsets。

​	**所以在这里作者并没有采用直接预测offset的方法，还是沿用了YOLO算法中直接预测相对于grid cell的坐标位置的方式。**

前面提到网络在**最后一个卷积层输出13*13大小的feature map，然后每个cell预测5个boundingbox，然后每个bounding box预测5个值：tx，ty，tw，th和to（这里的to类似YOLOv1中的confidence）**。

对这5个值进行如下计算：

​                                                                                  $$b_x = \sigma(t_x) + c_x$$

​                                                                                  $b_y = \sigma(t_y) + c_y​$

​                                                                                  $b_w = p_we^{t_w}​$

​                                                                                  $b_h = p_he^{t_h}​$

​                                                                         $Pr(object) * IOU(b, object) = \sigma(t_o)​$

​	其中 $t_x$和$t_y$经过sigmoid函数处理后范围在0到1之间，即边界框的中心位置会约束在当前cell内部，这样的归一化处理可以使模型训练更加稳定；$c_x$和$c_y$表示一个cell和图像左上角的横纵距离，在计算时每个cell的尺度为1；$p_w$和$p_h$表示先验框anchor box的宽度与长度，它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1；这样，$b_x$和$b_y$就是由距离图像左上角$c_x$和$c_y$的这个cell附近的anchor来预测$t_x$和$t_y​$得到的结果。

特征图的大小为（W，H）（论文中是（13，13）），则可以计算边界框相对于整张图片的位置和大小（4个值均在0和1之间）：

​                                                                 $b_x =( \sigma(t_x) + c_x)/W$

​                                                                 $b_y =( \sigma(t_y) + c_y)/H​$

​                                                                 $b_w = (p_we^{t_w})/W$

​                                                                  $b_h =( p_he^{t_h})/H$

​	如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的**最终位置和大小。**

​	这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。

![](D:\typora图片总\预测框计算.png)

**在上图中，$c_x$和$c_y$表示grid cell与图像左上角的横纵坐标距离，黑色虚线框是bounding box，蓝色矩形框就是预测的结果。**

**6、Fine-Grained Features（passthrough）**

​	YOLOv2的输入图片大小为416\*416，经过5次maxpooling之后得到13\*13大小的特征图，并以此特征图采用卷积做预测。13\*13大小的特征图对检测大物体是足够了，但是对于小物体还需要**更精细的特征图（Fine-Grained Features）**。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。

​	而YOLOv2提出了一种**passthrough层**来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26\*26大小的特征图（最后一个maxpooling层的输入），对于Darknet19模型来说就是大小为26\*26\*512的特征图。

​	passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2*2的局部区域，然后将其转化为channel维度，对于26\*26\*512的特征图，经**passthrough层**处理之后就变成了13\*13\*2048的新特征图（**特征图大小降低4倍，而channles增加4倍**，下图为一个实例），这样就可以**与后面的13\*13\*1024特征图连接在一起形成13\*13\*3072的特征图**，然后在此特征图基础上卷积做预测。

![](D:\typora图片总\passthrough层实例.png)

在网络中的具体连接结构如下：

![](D:\typora图片总\YOLOV2 passthrough.png)	

​	在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：

```python
out = tf.extract_image_patches(in, [1, stride, stride, 1], [1, stride, str
ide, 1], [1,1,1,1], padding="VALID")
// or use tf.space_to_depth
out = tf.space_to_depth(in, 2)
```

​	另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个1\*1卷积核进行卷积，然后再进行passthrough处理，这样26\*26\*512的特征图得到13\*13\*256的特征图。

​	**使用Fine-Grained Features之后，YOLOv2的性能有1%的提升。**

**7、Multi-Scale Training(多尺度训练)**

​	由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416\*416大小的图片。

​	为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations（迭代）之后改变模型的输入图片大小。由于YOLOv2的下采样（池化）总步长为32，输入图片大小选择一系列为32倍数的值：输入图片最小为320\*320，此时对应的特征图大小为10\*10（注意此时特征图大小不是奇数了）而输入图片最大为608\*608,对应的特征图大小为19\*19,在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。

![](D:\typora图片总\YOLOV2多尺度训练.png)

​	采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544,mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。 

![](D:\typora图片总\YOLOV2在VOC 2007数据集上的性能对比.png)

**8、Further Experiments**

​	作者在VOC2012上对YOLOv2进行训练，下图是和其他方法的对比。YOLOv2精度达到了73.4%，并且速度更快。同时YOLOV2也在COCO上做了测试（IOU=0.5），也和Faster RCNN、SSD作了成绩对比。总的来说，比上不足，比下有余。

![](D:\typora图片总\YOLOV2 VOC2012.png)

![](D:\typora图片总\YOLOV2 COCO.png)

##### Faster

​	YOLO一向是**速度和精度并重**，作者为了改善检测速度，也作了一些相关工作。

​	大多数检测网络有赖于VGG-16作为特征提取部分，VGG-16的确是一个强大而准确的分类网络，但是复杂度有些冗余。224 \* 224的图片进行一次前向传播，其卷积层就需要多达306.9亿次浮点数运算。在YOLO v1中，作者采用的训练网络是基于GooleNet，这里作者将GooleNet和VGG-16做了简单的对比，GooleNet在计算复杂度上要优于VGG16（8.25 billion operation VS 30.69 billion operation），但是前者在ImageNet上的top5准确率要稍低于后者（88% VS 90%）。

​	YOLOv2使用的是**基于Googlenet的定制网络Darknet-19**，比VGG-16更快，一次前向传播仅需85.2亿次运算。可是它的精度要略低于VGG-16，单张224 * 224取前五个预测概率的对比成绩为88%和90%（低一点也是可以接受的）。

**1、New Network：Darknet-19**

​	在YOLOV1中采用的GooleNet，包含24个卷积层和2个全连接层。而YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图所示：

![](D:\typora图片总\Darknet-19.png)

- Darknet-19与VGG-16模型设计原则是一致的，主要采用3*3卷积。

- 每经过一个2\*2的maxpooling层之后，**特征图维度降低为原来的一半，同时把特征图的通道数翻倍**。

- 借鉴了network in network的思想，Darknet-19最终采用了全局平均池化（global avg pooling），并且在3\*3卷积之间使用1\*1卷积来压缩特征图channles以降低模型计算量和参数。
- Darknet-19每个卷积层后面同样使用了batch normalization层以加快收敛速度，降低模型过拟合。
- 在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。

**2、Training for classification and Training for detection**

YOLOv2的训练主要包括三个阶段：

![](D:\typora图片总\YOLOV2训练的三个阶段.png)

第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为224\*224,共训练160个epochs。

第二阶段将网络的输入调整为448\*448,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1
准确度为76.5%，而top-5准确度为93.3%。

第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。

​	网络修改包括（网络结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个3\*3\*2014卷积层，同时增加了一个passthrough层，最后使用1\*1卷积层**输出预测结果**，输出的channels数为：    num_anchors x ( 5+ num_classes )，这个值和训练采用的数据集有关。已知num_anchor=5，则对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。

![](D:\typora图片总\YOLOV2输出tensor图.png)

以VOC数据集为例，最终的预测矩阵为T，如图其shape为（batch_size,13,13,125），可以先将其reshape为（batch_size,13,13,225），其中T[：，：，：，：，0：4]为边界框的位置和大小（$t_x,t_y,t_w,t_h$），T[：，：，：，：，4]为边界框的置信度，T[：，：，：，：，5：]为类别预测值。

下图是YOLOV2网络模型：

![](D:\typora图片总\YOLOV2训练1.png)

![](D:\typora图片总\YOLOV2网络模型.png)

![](D:\typora图片总\YOLOV2结构示意图.png)

我们已经知道YOLOv2的网络结构以及训练参数，但是原作者并没有给出YOLOv2的训练过程的两个最重要方面，即**先验框匹配（样本选择）以及训练的损失函数**。不过默认按照YOLOv1的处理方式也是可以处理的，在YOLO在TensorFlow上的实现darkflow（见yolov2/train.py），发现它就是如此处理的：

​	和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的5个先验框所对应的边界框负责预测它，具体是哪个边界框预测它，需要在训练中确定，即由那个与ground truth的IOU最大的边界框预测它，而剩余的4个边界框不与该ground truth匹配。**YOLOv2同样需要假定每个cell至多含有一个grounth truth**，而在实际上基本不会出现多于1个的情况。与ground truth匹配的先验框计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的边界框只计算置信度误差（此时target为0）。YOLOv2和YOLOv1的损失函数一样，为均方差函数。

​	但是根据YOLOv2的源码（训练样本处理与loss计算都包含在文件region_layer.c中，YOLO源码没有任何注释），并且参考国外的blog以及allanzelener/YAD2K（Ng深度学习教程所参考的那个Keras实现）上的实现，发现YOLOv2的处理比原来的v1版本更加复杂。先给出loss计算公式：

![](D:\typora图片总\YOLOV2训练损失函数.png)

​	**对于损失函数的解释：**

​	首先W,H分别指的是特征图（13*13）的宽与高，而A指的是先验框数目（这里是5），各个$\lambda$值是各个loss部分的权重系数。第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差。第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，在YOLOv1中target=1，而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的和误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale\* (2 - truth.w\*truth.h)，这样对于尺度较小的boxes其权重系数会更大一些，起到和YOLOv1计算平方根相似的效果（参考YOLO v2 损失函数源码分析）。

​	**最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升**。



##### Stronger

​	目前目标检测任务的一个限制就是数据集太小，相比于其他任务，例如分类、打标签，目标检测的数据集是有限的，分类是数百万的数据集，然而目标检测最多才数十万的数据集。这是因为，相比于分类任务的标注工作，目标检测的标注工作很复杂、很昂贵耗时的，导致目标检测数据与分类任务的数据大小之间整整差了几个数量级。

​	在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。**对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类（主要是精准的定位），而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。**

​	本文在YOLOV2的基础上，利用数据集整合方法以及联合训练算法在超过9000类的ImageNet分类数据以及COCO检测数据上进行训练，训练出YOLO9000模型。

​	作者选择在COCO和ImageNet数据集上进行联合训练，但是COCO数据集的分类是粗糙的，比如其分类就是猫或者狗等，但是ImageNet数据集的分类是细致的，比如泰迪、秋田等，不是大类的狗。传统的分类结构最后一层时候softmax，softmax的前提是label之间是互斥的。这里明显不满足这个条件。  所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构 WordTree，共有9418类。

​	结合COCO和ImageNet建立的WordTree如下图所示：

![](D:\typora图片总\wordtree数据集.jpg)

WordTree中的根节点为”physical object“，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。

![](D:\typora图片总\ImageNet与WordTree对比.jpg)

​	训练阶段检测数据和分类数据混合，如果输入是目标检测数据，则反向传播检测+分类的损失，如果输入是分类样本，只反向传播分类损失。在预测时，YOLOv2给出的置信度就是Pr（physicalobject），同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。

​	通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。

#### （3）关于YOLO V2-tiny

|            |           |             |             |            |            |
| ---------- | --------- | ----------- | ----------- | ---------- | ---------- |
| **number** | **layer** | **filters** | **size**    | **intput** | **output** |
| 1          | conv1     | 16          | 3x3 / 1     | 416x416x3  | 416x416x16 |
| 2          | max1      |             | 2x2 / 2     | 416x416x16 | 208x208x16 |
| 3          | conv2     | 32          | 3x3 / 1     | 208x208x16 | 208x208x32 |
| 4          | max2      |             | 2x2 / 1     | 208x208x32 | 104x104x32 |
| 5          | conv3     | 64          | 3x3 / 1     | 104x104x32 | 104x104x64 |
| 6          | max3      |             | 2x2 / 2     | 104x104x64 | 52x52x64   |
| 7          | conv4     | 128         | 3x3 / 1     | 52x52x64   | 52x52x128  |
| 8          | max4      |             | 2x2 / 2     | 52x52x128  | 26x26x128  |
| 9          | conv5     | 256         | 3x3 / 1     | 26x26x128  | 26x26x256  |
| 10         | max5      |             | 2x2 / 2     | 26x26x256  | 13x13x256  |
| 11         | conv6     | 512         | 3x3 / 1     | 13x13x256  | 13x13x512  |
| 12         | max6      |             | **2x2 / 1** | 13x13x512  | 13x13x512  |
| 13         | conv7     | 1024        | 3x3 / 1     | 13x13x512  | 13x13x1024 |
| 14         | conv8     | 1024        | 3x3 / 1     | 13x13x1024 | 13x13x1024 |
| 15         | conv9     | 125         | 1x1 / 1     | 13x13x1024 | 13x13x125  |
| 16         | detection |             |             |            |            |

**此为YOLOV2-tiny网络架构，共有9个卷积，6个最大池化层，注意，第十层的池化步长是1**

### **3、YOLOv3**

#### **（1）概述**

​	YOLO算法的基本思想：首先通过特征提取网络对输入图像提取特征，得到一定size的feature map，然后将输入图像分成13\*13个grid cell，接着如果ground truth中某个object的中心坐标落在哪个grid cell中，那么就由该grid cell来预测object，因为每个grid cell都会预测固定数量的bounding box（YOLO V1中是2个，YOLO V2中是5个，YOLO V3中是3个，这几个bounding box的初始size是不一样的），这几个bounding box中只有和ground truth的IOU最大的bounding box才是用来预测该object的。可以看出预测得到的输出feature map有两个维度是提取到的特征的维度，比如13\*13，还有一个维度（深度）是B*（5+C）。

​	相比YOLOv2，YOLOv3最大的变化包括两点：使用残差模型和采用FPN架构。

- **YOLOv3的特征提取器是一个残差模型，因为包含53个卷积层，所以称为Darknet-53，从网络结构上看，相比Darknet-19网络使用了残差单元，所以可以构建得更深。**
- **另外一个点是采用FPN架构（Feature Pyramid Networks for Object Detection）来实现多尺度检测。YOLOv3采用了3个尺度的特征图（当输入为416\*416时）：(13\*13)，(26\*26)，(52*52），VOC数据集上的YOLOv3网络结构如图，其中红色部分为各个尺度特征图的检测结果。YOLOv3每个位置使用3个先验框，所以使用kmeans得到9个先验框，并将其划分到3个尺度特征图上，尺度更大的特征图使用更小的先验框，和SSD类似。**

  **YOLOV3的结构图如下：**

**![](D:\typora图片总\YOLOV3结构图.jpg)**

**对上图的补充解释：**

**DBL：如上图左下角，即代码中的Darknetconv2d_BN_Leaky，是yolo v3的基本组件，其结构为卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。**

**resn：n代表数字，有res1，res2，.....res8等等，表示这个res_block里含有多少个res_unit。这是YOLOV3的大组件，YOLO V3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深（从v2的darknet-19上升到v3的darknet-53，前者没有残差结构）**

**concat：张量拼接，即将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。**

**YOLOV3网络层：**

**![](D:\typora图片总\YOLOV3 网络层.png)**

​	如上图所示：对于代码层面的layers数量一共有252层，包括add层23层(主要用于res_block的构成，每个res_unit需要一个add层，一共有1+2+8+8+4=23层)。除此之外，BN层和LeakyReLU层数量完全一样(72层)，在网络结构中的表现为：每一层BN后面都会接一层LeakyReLU。卷积层一共有75层，其中有72层后面都会接BN+LeakyReLU的组合构成基本组件DBL。看结构图，可以发现上采样和concat都有2次，和表格分析中对应上。每个res_block都会用上一个零填充，一共有5个res_block。

​	

​	 从YOLO的三代变革中可以看到，在目标检测领域比较好的策略有：设置先验框、采用全卷积做预测（去除了pooling）、采用残差网络、采用多尺度特征图做预测。原作者还做了其他几种尝试，但是效果不好，如：

- **Anchor box x; y offset predictions.We tried using the normal anchor box prediction mechanism where you predict the x; y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn’t work very well.**
- **Linear x; y predictions instead of logistic. We tried using a linear activation to directly predict the x; y offset instead of the logistic activation. This led to a couple point drop in mAP.**
- **Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren’t totally sure.**

#### **（2）实现方法**

**1、Bounding Box Prediction**

​	bounding box的坐标预测方式还是延续了YOLO v2的做法，简单讲就是下面的公式，$t_x,t_y,t_w,t_h$ 就是模型的预测输出。$c_x,c_y$表示grid cell的坐标。比如某层的feature map的大小是13\*13，那么grid cell就是13\*13个，第0行第1列的grid cell的坐标$c_x$就是1，$c_y$就是0。$p_w$和$p_h$表示预测前bounding box的size。$b_x、b_y、b_w、b_h$就是预测得到的bounding box的中心的坐标和size。坐标的损失采用的是平方误差损失(sum of squared error loss)。

​                                                                                 $$b_x = \sigma(t_x) + c_x$$

​                                                                                  $b_y = \sigma(t_y) + c_y$

​                                                                                  $b_w = p_we^{t_w}$

​                                                                                  $b_h = p_he^{t_h}$

​                                                                         $Pr(object) * IOU(b, object) = \sigma(t_o)$

**![](D:\typora图片总\YOLOV2坐标.jpg)**

​	YOLOV3利用logistics regression为每个bounding box预测objectness score（Faster RCNN中的RPN）。如果预测的bounding box与ground truth重叠（overlaps： IOU？？）的地方最多，则objectness score为1；如果预测的bounding box不是最好的，但是和ground truth object的重叠也超过了一个阈值threshold，（遵循Faster RCNN）则忽略这个bounding box（不奖励也不惩罚的机制）。与Fatser RCNN中不同的是，YOLOV3为每个ground truth只分配一个bounding box。如果这个bounding box没有被分配给ground truth，则再算loss的时候不会计算这个bounding box的coordinate以及class predictions的loss，只计算objectness score loss。

**2、Class Prediction**

​	类别预测方面主要是将原来的单标签分类改进为多标签分类，因此网络结构上就将原来用于单标签多分类的softmax层换成用于多标签多分类的逻辑回归层。首先说明一下为什么要做这样的修改，原来分类网络中的softmax层都是假设一张图像或一个object只属于一个类别，但是在一些复杂场景下，一个object可能属于多个类，比如你的类别中有woman和person这两个类，那么如果一张图像中有一个woman，那么你检测的结果中类别标签就要同时有woman和person两个类，这就是多标签分类，需要用逻辑回归层来对每个类别做二分类。逻辑回归层主要用到sigmoid函数，该函数可以将输入约束在0到1的范围内，因此当一张图像经过特征提取后的某一类输出经过sigmoid函数约束后如果大于0.5，就表示属于该类。

**3、Predictions Across Scales**

​	YOLO v3采用多个scale融合的方式做预测。原来的YOLO v2有一个层叫：passthrough layer，假设最后提取的feature map的size是13\*13，那么这个层的作用就是将前面一层的26\*26的feature map和本层的13\*13的feature map进行连接，有点像ResNet。当时这么操作也是为了加强YOLO算法对小目标检测的精确度。这个思想在YOLO v3中得到了进一步加强，在YOLO v3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26\*26和52\*52），在多个scale的feature map上做检测，对于小目标的检测效果提升还是比较明显的。前面提到过在YOLOv3中每个grid cell预测3个bounding box，看起来比YOLO v2中每个grid cell预测5个bounding box要少，其实不是！因为YOLO v3采用了多个scale的特征融合，所以boundign box的数量要比之前多很多，以输入图像为416\*416为例：（13\*13+26\*26+52\*52）\*3和13\*13\*5相比哪个更多应该很清晰了。

**关于bounding box的初始尺寸还是采用YOLO v2中的kmeans聚类的方式来做，这种先验知识对于bounding box的初始化帮助还是很大的，毕竟过多的bounding box虽然对于效果来说有保障，但是对于算法速度影响还是比较大的。作者在COCO数据集上得到的9种聚类结果：(10\*13); (16\*30);(33\*23); (30\*61); (62\*45); (59\*119); (116\*90); (156\*198); (373\*326)，这应该是按照输入图像的尺寸是416\*416计算得到的。**

**![](D:\typora图片总\YOLOV3预测.png)**

**对YOLOV3输出的补充解释：**

**第一点， 9个anchor会被三个输出张量平分的。根据大中小三种size各自取自己的anchor。**

**第二点，每个输出y在每个自己的网格都会输出3个预测框，这3个框是9除以3得到的，这是作者设置的，我们可以从输出张量的维度来看，13x13x255。255是怎么来的呢，3\*(5+80)。80表示80个种类，5表示位置信息和置信度，3表示要输出3个prediction。在代码上来看，3*(5+80)中的3是直接由num_anchors//3得到的。**

**第三点，作者使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。**

**根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。**

**4、Feature Extractor**

​	网络结构（Darknet_53）一方面基本采用全卷积（YOLO v2中采用pooling层做feature map的 sample，这里都换成卷积层来做了），另一方面引入了residual结构（YOLO_v2中还是类似VGG那样直筒型的网络结构，层数太多训起来会有梯度问题，所以Darknet_19也就19层，因此得益于 ResNet的residual结构，训深层网络难度大大减小，因此这里可以将网络做到53层，精度提升比较明显）。Darknet53只是特征提取层，源码中只使用了pooling层前面的卷积层来提取特征，因此multiscale的特征融合和预测支路并没有在该网络结构中体现，具体信息可看源码：https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg

​	预测支路采用的也是全卷积的结构，其中最后一个卷积层的卷积核个数是255，是针对COCO数据集的80类：

**3*(80+4+1)=255，3表示一个grid cell包含3个bounding box，4表示框的4个坐标信息，1表示objectness score。**

**模型训练方面还是采用原来YOLO v2中的multiscale training。**

**![](D:\typora图片总\YOLOV3结构图1.jpg)**

**![](D:\typora图片总\YOLOV3结构图2.jpg)**

**5、Training**

**模型训练方面还是对整张图进行训练，没有进行难样本挖掘等。并且还是采用原来YOLO v2中的multiscale training，许多data augmentation，batch normalization等。**

**6、loss function**

​	对掌握Yolo来讲，loss function不可谓不重要。在v3的论文里没有明确提所用的损失函数，确切地说，yolo系列论文里面只有yolo v1明确提了损失函数的公式。对于yolo这样一种讨喜的目标检测算法，就连损失函数都非常讨喜。在v1中使用了一种叫sum-square error的损失计算方法，就是简单的差方相加而已。想详细了解的可以看我关于v1解释的博文。我们知道，在目标检测任务里，有几个关键信息是需要确定的:

​                                      （x，y）（x，y），class，confidence

​	根据关键信息的特点可以分为上述四类，损失函数应该由各自特点确定。最后加到一起就可以组成最终的loss_function了，也就是一个loss_function搞定端到端的训练。可以从代码分析出v3的损失函数，同样也是对以上四类，不过相比于v1中简单的总方误差，还是有一些调整的：

```python
xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[..., 0:2],
                                                                       from_logits=True)
wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh - raw_pred[..., 2:4])
confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits=True) + 
                          (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5],
                                                                    from_logits=True) * ignore_mask
class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[..., 5:], from_logits=True)

xy_loss = K.sum(xy_loss) / mf
wh_loss = K.sum(wh_loss) / mf
confidence_loss = K.sum(confidence_loss) / mf
class_loss = K.sum(class_loss) / mf
loss += xy_loss + wh_loss + confidence_loss + class_loss
```

​	以上是一段keras框架描述的yolo v3 的loss_function代码。忽略恒定系数不看，可以从上述代码看出：除了w, h的损失函数依然采用总方误差之外，其他部分的损失函数用的是二值交叉熵。最后加到一起。这个binary_crossentropy就是一个最简单的交叉熵，一般用于二分类，这里的两种二分类类别可以理解为"对和不对"这两种。

#### **（3）性能分析**

**![](D:\typora图片总\YOLO V3.png)**

- **此图是YOLO V3在COCO测试集与其他检测算法的对比图，可以看到，YOLO V3的速度是最快的，虽然其AP值并不是最好的。**



**![](D:\typora图片总\YOLO V3性能.png)**

- **此图是几个网络在ImageNet数据集上的性能和精度对比。可以看出Darknet53的性能还是非常不错的。**



**![](D:\typora图片总\YOLO V3性能2.jpg)**

- **此图是YOLO V3的实验结果。原来YOLO v2对于小目标的检测效果是比较差的，通过引入多尺度特征融合的方式，可以看出YOLO v3的APS要比YOLO v2的APS高出不少。**

  

**![](D:\typora图片总\YOLO V3性能3.jpg)**

- **此图中，All the other slow ones代表其他算法，可以看到YOLO V3的运行速度较其他算法确实很快。**

### **4、关于YOLO实践**

**darkflow：tensorflow实现的darknet（Python 版本YOLO）**

**Darkflow下载地址：https://github.com/thtrieu/darkflow**

**Darknet下载地址：https://github.com/pjreddie/darknet**

**YOLO训练模型下载：https://pjreddie.com/darknet/yolo/**



**darknet源码阅读：**

首先打开darknet文件夹，查看源码目录：

![](D:\typora图片总\darknet源码.png)

yolo主函数main在examples/darknet.c下

根据yolo训练的命令：

**./darknet detector train cfg/voc.data cfg/yolo-voc.cfg darknet19_448.conv.23**   

- 可以看出，main主函数中的参数argv[]的对应值分别是 argv[0] -> darknet argv[1] -> detector argv[2] -> train .....

- 主函数就是对于参数argv[1]的一个判断，根据argv[1]的内容来启动不同的程序。

- 继续跟着训练命令走argv[1] = detector时，调用的函数是run_detector，而这个函数在examples/detector.c的最后。

- run_detector的主要作用是在根据argv[]的值执行不同的函数，其他关于gpu，threshold之类的都可以不用管，这里最重要的是argv[2]的值，根据其值的不同，执行不同函数，这里的test_detector,train_detector这些函数在detector.c中都有定义，并且从名字上我们就可以看出这些函数是干什么的。这里我们依旧跟随之前的训练命令，argv[2] = train，这里继续看一下train_detector函数
- 这里我们主要重视的函数是第7行的read_data_cfg，第8行的train_images，第9行的backup_directory和第25行的load_network函数：
  read_data_cfg中的参数datacfg在run_detector中可以看出就是arg[3]，在本例中对应的就是voc.data
  train_images是用来指定所要训练的图片集的路径的。
  backup_directory是用来指定训练出来的权值的路劲的。而load_network是用来载入所要训练的网络结构和参数的，这里run_detector中可以看出load_network的参数之一cfgfile就是argv[4]，在我们这个例子中也便就是yolo-voc.cfg
- 再看一下cfg/voc.data（这里修改过）

```c++
classes= 2
train  = /home/iair339-04/darknet/scripts/train.txt
valid  = /home/iair339-04/darknet/scripts/2007_test.txt
names = data/kitti.names
backup = backup
```

​       这里可以看出voc.data是用来指定类别数classes，训练集路径train，测试集路径valid和类别名称names和备份文件路径backup的





yolo测试代码梳理：https://blog.csdn.net/baobei0112/article/details/80075128

yolo v2文件结构和源码的简单梳理：https://blog.csdn.net/flztiii/article/details/74274997

https://blog.csdn.net/jocelyn870/article/details/78931829

https://blog.csdn.net/ywcpig/article/details/79926064

https://blog.csdn.net/samylee/article/details/51684856

https://blog.csdn.net/liuxiaodong400/article/details/80867229

https://blog.csdn.net/runner668/article/details/80579063**

tiny-yolov2训练及测试：https://www.jianshu.com/p/84867584bffb

darknet执行详解：https://blog.csdn.net/qq_29893385/article/details/81261339***

## **七、SSD**







## **注：文档链接**

**1、two-stage类目标识别模型讲解：**

**https://blog.csdn.net/julialove102123/article/details/79894755**

**https://blog.csdn.net/sum_nap/article/details/80388110**

**2、关于one-stage 目标检测解析（很全面，但是生肉英文）：**

**http://machinethink.net/blog/object-detection/**

3、关于darknet：

**darknet源码及分析：**  **https://github.com/hgpvision/darknet** 

**源码阅读解析：https://blog.csdn.net/gzj2013/article/details/84837198** 

**最全YOLO执行教程：<https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects** 

**4、关于YOLO V1**

**YOLOV1源码（tensorflow）：https://github.com/hizhangp/yolo_tensorflow** 

**匹配的知乎解答：https://zhuanlan.zhihu.com/p/25053311**

**源码网站：http://pjreddie.com/yolo/**

**arxiv：http://arxiv.org/abs/1506.02640**

**注：为了防止自己的idea在论文被收录前被别人剽窃，我们会将预稿上传到arvix作为预收录，因此这就是个可以证明论文原创性（上传时间戳）的文档收录网站 。**

**code：http://pjreddie.com/darknet/yolo/**

**github：https://github.com/pjreddie/darknet**

**blog：https://blog.csdn.net/hrsstudy/article/details/70305791**重点**

**https://blog.csdn.net/m0_37192554/article/details/81092514**

**https://www.cnblogs.com/cvtoEyes/p/8608205.html**

**https://pjreddie.com/publications/yolo/**

**slides：**

**https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p **重点**

**reddit:**

**https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/**

**github代码：**

**https://github.com/gliese581gg/YOLO_tensorflow**

**https://github.com/xingwangsfu/caffe-yolo**

**https://github.com/frankzhangrui/Darknet-Yolo**

**https://github.com/BriSkyHekun/py-darknet-yolo**

**https://github.com/tommy-qichang/yolo.torch**

**https://github.com/frischzenger/yolo-windows**

**https://github.com/AlexeyAB/yolo-windows**

**https://github.com/nilboy/tensorflow-yolo**

**关于yolov1原理解释的幻灯片（需要翻墙）：**

**[https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000#slide=id.g137784ab86_4_1509**

**5、关于YOLO V2**

**源码网站：http://pjreddie.com/yolo9000/**

**arxiv：https://arxiv.org/abs/1612.08242**

**code：http://pjreddie.com/darknet/yolo/**

**github(Chainer)：https://github.com/leetenki/YOLOv2**

**YOLOV2参数详解：https://blog.csdn.net/xiaoye5606/article/details/72845051**

**YOLOV2源码分析：** **https://blog.csdn.net/qq_17550379/column/info/18380** 

**博客讲解：https://blog.csdn.net/zijin0802034/article/details/77097894**

**https://blog.csdn.net/l7H9JA4/article/details/79955903**内有部分源码**

**https://blog.csdn.net/u014380165/article/details/77961414**

**https://blog.csdn.net/Jesse_Mx/article/details/53925356**

**https://cloud.tencent.com/developer/article/1156245**

**github(Keras)：https://github.com/allanzelener/YAD2K**

**github(PyTorch)：https://github.com/longcw/yolo2-pytorch**

**github(Tensorflow)：https://github.com/hizhangp/yolo_tensorflow**

**github(Windows)：https://github.com/AlexeyAB/darknet**

**github：https://github.com/choasUp/caffe-yolo9000**

**github：https://github.com/philipperemy/yolo-9000**

**6、关于YOLO V3**

**论文链接：https://pjreddie.com/media/files/papers/YOLOv3.pdf**

**YOLOv3论文地址：https://arxiv.org/abs/1804.02767**

**源码code：https://pjreddie.com/yolo/**

**pytorch: YOLOv3实现 https://blog.csdn.net/l7H9JA4/article/details/80655711**

**博客：https://blog.csdn.net/u014380165/article/details/80202337**

​          https://blog.csdn.net/wfei101/article/details/80011474

​          https://blog.csdn.net/leviopku/article/details/82660381**重点

**blog：https://blog.paperspace.com/tag/series-yolo/，其中part1是介绍YOLO算法相关的基础知识，part2到part5是介绍如何用PyTorch实现YOLO v3算法，非常推荐**

**YOLO V3网络结构分析：https://blog.csdn.net/qq_37541097/article/details/81214953**

**关于yolov3的部分解读：https://xmfbit.github.io/2018/04/01/paper-yolov3/**

**yolov3 pytorch教程：**

-  **英文原版生肉（大概能解决你一半以上关于Yolov3的困惑）**


**<https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/>**

-  **知乎翻译中文**


**<https://zhuanlan.zhihu.com/p/36899263>**

-  **对应github（快去给他小星星吧）**


**<https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch>**

**解释了yolov3的forward和anchor box相关内容（解释了源码yolo_layer.c line195~237，待验证）：**

**https://blog.csdn.net/weixin_41015185/article/details/84189515**

**7、YOLO_tiny**

**yolo_v2_tiny：https://github.com/simo23/tinyYOLOv2**

**YOLOV2-TINY-VOC.cfg 参数解析：https://www.cnblogs.com/MY0213/p/9821381.html**

**yolo_v3_tiny：https://github.com/khanhhhh/tiny-yolo-tensorflow**

**8、目标检测SSD讲解：https://zhuanlan.zhihu.com/p/33544892**

**9、经典论文原文及其中文对照翻译： Alexnet   VGG  Resnet  GoogLenet  BN-GoogLenet  Inception-v3  SENet**

**YOLO  SSD  YOLO9000  Deformable-ConvNets  Faster R-CNN  R-FCN  FPN CRNN  CTPN**

**https://github.com/jiajunhua/SnailTyan-deep-learning-papers-translation**

**10、车道线检测的超全资料集锦：**

**<https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247486508&idx=1&sn=9164c7331acd81de86c02c5ed3c15c9a&chksm=f9a27ea3ced5f7b5137a66e699c0c481e4215bb0548a46935a6e801295043932114daad65226&mpshare=1&scene=23&srcid=0118ttA6qHkcqai87mHUlHGJ#rd>**

**11、关于边框回归（anchor box）讲解：https://blog.csdn.net/zijin0802034/article/details/77685438**

**12、YOLO\SSD代码：https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/ObjectDetections**

**13、详细讲述了BN的作用：https://arxiv.org/abs/1502.03167**

